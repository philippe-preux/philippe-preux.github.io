<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr" lang="fr">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <title>Apprentissage par renforcement, M2 Informatique, Université de Lille</title>
  <link href="https://philippe-preux.github.io/css/ma.css" 
	rel="stylesheet" type="text/css" media="all">
  <link rel="shortcut icon" type="image/x-icon" 
	href="https://philippe-preux.github.io/img/site.ico">
</head>

<body vlink="#551A8B">

<div class="ensg">
<div class="titreDuModule">
  Apprentissage par renforcement
</div>
<div class="formation">
  M2 Informatique<br>
  Université de Lille<br>
  2024-2025
</div>

<ul>
  <li><a href="#obj">Objectifs de ce module</a></li>
  <li><a href="#calendrier">Calendrier</a></li>
  <li><a href="#tps">Mise en pratique</a></li>
  <li><a href="#cc">Contrôle de connaissances</a></li>
  <li><a href="#pr">Pré-requis</a></li>
  <li><a href="#biblio">Biblio</a></li>
</ul>

<h3 id="obj">Objectifs de ce module</h3>

<p>

  Ce module est la suite du module intitulé <a href="../PDI/">problèmes de décision de Markov</a> (PDI). Dans le module PDI, on suppose que l'incertitude est connue. Dans le cadre de l'apprentissage par renforcement, on lève cette hypothèse. L'agent doit désormais découvrir par l'expérience (= apprendre) comment son environnement réagit à ses actions afin de déterminer une politique optimale.

  <br>

  Il existe de nombreux algorithmes pour résoudre ce type de problèmes. 
  On étudiera les principales idées et les principaux algorithmes qui permettent de résoudre les problèmes d'apprentissage par renforcement.

</p>

<h3 id="calendrier">Calendrier</h3>

<p>

Les séances se déroulent les lundis matins de 8h à midi.

</p>

<h3 id="tps">Mise en pratique</h3>

<ul>
  <li><a href="./tps/Q-learning">Méthodes basées sur la différence temporelle</a>.</li>
  <li><a href="./tps/discrétisation">Discrétisation de l'espace d'états</a>.</li>
  <li><a href="./tps/fqi"><i>Neural Fitted Q-Iteraton</i></a>.</li>
</ul>

<h3 id="cc">Contrôle de connaissances</h3>

<p>

  Le contrôle de connaissances est continu. Les contrôles sont annoncés. Dates&nbsp;:
  
</p>

<ul>
  <li>lundi 2 décembre 2024. Au programme&nbsp;: tout ce qui a été fait depuis le début du module APR, en cours, en TP et dans le <a href="https://philippe-preux.github.io/Documents/digest-ar.pdf">poly (jusque la section 5.2)</a>.</li>
  <li>lundi 16 décembre 2024.</li>  
</ul>

<p>

  Notes de cours, poly et TP autorisés pendant les épreuves.
  
</p>

<h3 id="pr">Pré-requis</h3>

<ul>
  <li>le cours «&nbsp;problème de décision de Markov&nbsp;».</li>
</ul>

<h3 id="biblio">Biblio</h3>

<ul>
  <li>Mon <a href="https://philippe-preux.github.io/Documents/digest-ar.pdf">polycopié de cours</a>.</li>
</ul>

</body>
</html>
