<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr" lang="fr">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <title>Apprentissage par renforcement, M2 Informatique, Université de Lille</title>
  <link href="https://philippe-preux.github.io/css/ma.css" 
	rel="stylesheet" type="text/css" media="all">
  <link rel="shortcut icon" type="image/x-icon" 
	href="https://philippe-preux.github.io/img/site.ico">
</head>

<body vlink="#551A8B">

<div class="ensg">
<div class="titreDuModule">
  Apprentissage par renforcement
</div>
<div class="formation">
  M2 Informatique<br>
  Université de Lille<br>
  2024-2025
</div>

<ul>
  <li><a href="#obj">Objectifs de ce module</a></li>
  <li><a href="#calendrier">Calendrier</a></li>
  <li><a href="#tps">Mise en pratique</a></li>
  <li><a href="#cc">Contrôle de connaissances</a></li>
  <li><a href="#pr">Pré-requis</a></li>
  <li><a href="#biblio">Biblio</a></li>
</ul>

<h3 id="obj">Objectifs de ce module</h3>

<p>

  Ce module est la suite du module intitulé <a href="../PDI/">problèmes de décision de Markov</a> (PDI). Dans le module PDI, on suppose que l'incertitude est connue. Dans le cadre de l'apprentissage par renforcement, on lève cette hypothèse. L'agent doit désormais découvrir par l'expérience (= apprendre) comment son environnement réagit à ses actions afin de déterminer une politique optimale.

  <br>

  Il existe de nombreux algorithmes pour résoudre ce type de problèmes. 
  On étudiera les principales idées et les principaux algorithmes qui permettent de résoudre les problèmes d'apprentissage par renforcement.

</p>

<h3 id="calendrier">Calendrier</h3>

<p>

Les séances se déroulent les lundis matins de 8h à midi.

</p>

<h3 id="tps">Mise en pratique</h3>

<ul>
  <li><a href="./tps/Q-learning">Méthodes basées sur la différence temporelle</a>.</li>
  <li><a href="./tps/discrétisation">Discrétisation de l'espace d'états</a>.</li>
  <li><a href="./tps/fqi"><i>Neural Fitted Q-Iteration</i></a>.</li>
  <li>Implantation DQN&nbsp;: voir le polycopié pour la description de l'algorithme et le tester sur les deux problèmes traités dans les deux TPs précédents.</li>
</ul>

<h3 id="cc">Contrôle de connaissances</h3>

<p>

  Le contrôle de connaissances est continu. Les contrôles sont annoncés.

  <br/>

  Dates&nbsp;:
  
</p>

<ul>
  <li>lundi 2 décembre 2024. Au programme&nbsp;: tout ce qui a été fait depuis le début du module APR, en cours, en TP et dans le <a href="https://philippe-preux.github.io/Documents/digest-ar.pdf">poly (jusque la section 5.2)</a>.</li>
  <li>lundi 16 décembre 2024. <a href="Prénom-Nom.cc2.odt">le sujet au format .odt</a> (<a href="Prénom-Nom.cc2.docx">format .docx).</a></li>  
</ul>

<p>

  Notes de cours, poly et TP autorisés pendant les épreuves.
  
</p>

<h3 id="pr">Pré-requis</h3>

<ul>
  <li>le cours «&nbsp;problème de décision de Markov&nbsp;».</li>
</ul>

<h3 id="biblio">Biblio</h3>

<ul>
  <li>Mon <a href="https://philippe-preux.github.io/Documents/digest-ar.pdf">polycopié de cours</a>.</li>
  <li>Voir <a href="https://www.youtube.com/watch?v=Lt-KLtkDlh8">cette vidéo avec un vrai <i>cartpole</i> par l'équipe de Riedmiller, qui a proposé FQI.</li>
</ul>

<p>

  Faute de temps, nous n'avons pas traité des méthodes d'apprentissage direct de politique et les acteurs-critiques. Celles-ci sont décrites dans mon polycopié. Il faut comencer par l'apprentissage direct de politique et l'algorithme REINFORCE. Ensuite, on continue avec les acteurs-critiques, en particulier DDPG, TD3 mais surtout PPO et SAC qui sont actuellement, avec DQN, les meilleurs algorithmes d'apprentissage par renforcement. On pourra aussi consulter&nbsp;:
  
</p>

<ul>
  <li><a href="https://spinningup.openai.com/en/latest/">ce site</a>, en particulier les parties comprises entre <i>Vanilla Policy Gradient</i> et <i>Soft Actor-Critic</i>.</li>
  <li><a href="https://medium.com/@brianpulfer/ppo-intuitive-guide-to-state-of-the-art-reinforcement-learning-410a41cb675b">À propos de PPO</a></li>
  <li><a href="https://stable-baselines3.readthedocs.io/en/master/">Ici, des explications et des implantations de bonne qualité.</a></li>
  <li>L'école d'été que nous avons organisée en 2019&nbsp;: <a href="https://rlss.inria.fr/">des tas de choses sur l'apprentissage par renforcement, des explications et des TP.</a></li>
</ul>

</body>
</html>
