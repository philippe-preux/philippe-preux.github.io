<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" >
    <title><i>Neural Fitted Q-Iteration</i></title>
    <link href="/home/ppreux/philippe-preux.github.io/css/ma.css"
	  rel="stylesheet" type="text/css" media="all" >
    <!--link href="https://philippe-preux.github.io/css/ma.css"
	  rel="stylesheet" type="text/css" media="all" -->
    <link rel="shortcut icon" type="image/x-icon"
	  href="https://philippe-preux.github.io/img/site.ico" >
    <!--link rel="shortcut icon" type="image/x-icon"
	  href="/home/ppreux/philippe-preux.github.io/img/site.ico" -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
      div.c {
	  background-color: #bafcba;
      }
      div.python {
	  background-color: #c0c0c0;
      }
      table.tr.td.c {
	  background-color: #bafcba;
	  width: 50%;
      }
      table.tr.td.python {
	  background-color: #c0c0c0;
	  width: 50%;
      }

      /* Ce qui suit fait ce que je voulais faire en mettant des attributs aux balises td, ce qui n'est pas compatible avec HTML5. Je voulais une table avec deux colonnes de même largeur, chacune avec une couleur de fond particulière.
       J'ai trouvé cette solution là : https://www.w3schools.com/howto/howto_css_two_columns.asp */
      * {
	  box-sizing: border-box;
      }
      /* Create two equal columns that floats next to each other */
      .columnC {
	  background-color: #bafcba;
	  float: left;
	  width: 50%;
	  padding: 10px;
      }
      .columnP {
	  background-color: #c0c0c0;
	  float: left;
	  width: 50%;
	  padding: 10px;
      }
      /* Clear floats after the columns */
      .row:after {
	  content: "";
	  display: table;
	  clear: both;
      }
      /* ça marche bien pour la largeur et la couleur de fond, mais pas pour
	 la hauteur. Après avoir qwanté sans trouver de solution, j'ai défini
	 la classe div ci-dessous : c'est affreux, mais ça marche comme
	 disent les étudiants ! (-(
	 Mais il faut faire l'ajustement à la main. C'est bien pour cela que
	 « non, ça ne marche pas !», mais ça rend ce que je voulais donc en
	 attendant de trouver mieux.
     */
      div.theUglyTrick {
	  visibility:hidden;
      }
    </style>
  </head>
  
<body>

<div class="tpR">

  <h1><i>Neural Fitted Q-Iteration</i></h1>

  <h2>Introduction</h2>

  <p>

    On poursuit l'étude du cas des espaces d'états infinis, non dénombrables.

    <br>

    Dans ce TP, on va s'intéresser à un algorithme très simple bien adapté à ce type de problèmes, le <i>Neural Fitted Q-iteration</i>, qui a été introduit par M. Riedmiller en 2005 dans <a href="https://link.springer.com/content/pdf/10.1007/11564096_32.pdf">ce papier</a> que je vous encourage à lire.

    <br/>

    Comme dans le TP précédent, on utilise le pendule inversé et la voiture dans la montagne.
    
  </p>

  <h2>Algorithme FQI</h2>

  <p>
    

  </p>

  <ol>
    <li>Initialiser l'algorithme&nbsp;; prendre une politique initiale \( \pi_0 \) aléatoire&nbsp;; \( k \gets 0 \).</li>
    <li><b>Répéter un certain nombre de fois&nbsp;:</b>
      <ol>
	<li>collecter des données en exécutant plusieurs épisodes avec pi_0 \) et en recueillant les quadruplets \( (e_t, a_t, r_t, e_{t+1}) \). On appelle \( {\cal D}_k \) cet ensemble de quadruplets.</li>
	<!--li><b>Répéter un certain nombre de fois&nbsp;:</b>
	  <ol>
	    <li>échantillonner un lôt d'une centaine de données depuis \( {\cal D} \).</li>
	    <li>constituer un jeu d'exemples d'apprentissage. Chaque exemple est un couple \( ((e_t, a_t), q_t = r_t + \gamma \max_{a'}{\hat{Q_k} (e_{t+1},a'} \).</li>
	    <li>Régresser ce jeu d'exemples. On obtiendra les paramètres du modèle de régression&nbsp;: \( \theta_{k+1} \gets \mbox{arg min} \sum (q_t - \hat{Q_k} (e_t, a_t))^2 \)</li>
	  </ol>
	</li-->
	  <li>à partir \( {\cal D}_k \), constituer un jeu d'exemples d'apprentissage dans lequel chaque exemple est un couple \( ((e_t, a_t), q_t = r_t + \gamma \max_{a'}{\hat{Q_k} (e_{t+1},a'} \).</li>
	<li>Régresser ce jeu d'exemples. On obtiendra les paramètres du modèle de régression \( \hat{Q_{k+1}} \)&nbsp;: \( \theta_{k+1} \gets \mbox{arg min} \sum (q_t - \hat{Q_k} (e_t, a_t))^2 \)</li>
	<li>\( k++ \)</li>
      </ol>
    </li>
  </ol>

  <h2>Implantation</h2>

  <p>

    <span class="alertje">Vos expériences doivent être reproductibles.</span>
    
  </p>

  <p>

    <b>À faire&nbsp;:</b> implanter cet algorithme.
    Encore une fois, faire en sorte que l'implantation soit générique.

    <br>

    Le régresseur sera un perceptron multi-couches.
    
    <div class="row">
      <div class="columnC" >
	<b>C</b>
	<div class="c">
	  <p>
	    On utilise <a href="https://github.com/libfann/fann">cette bilbliothèque logicielle</a>. Elle est installée sur les ordinateurs en salle TP. Sur votre ordinateur personnel, il faut peut-être l'installer.

	    <br>
	    
	    Pour compiler avec <code>gcc</code>, on spécifiera l'option <code>-lfann</code>.

	    <br>

	    La documentation est <a href="https://xxxxxxxxxxxxxx/fann/docs/html/files/fann-h.html">là</a>.

	    <br>

	    Je donne un petit exemple d'utilisation de cette bibliothèque dans <a href="./exemple-fann.tgz">cette archive</a> pour une tâche de classification supervisée.
	  </p>
	</div>
      </div>
      <div class="columnP" >
	<b>Python</b>
	<div class="python">
	  <p>
	    
	    On utilise un <kbd>MLPRegressor</kbd> de la bibliothèque <code>scikit_learn</code>. Pour cette application, je vous conseille d'utiliser l'option <kbd>solver = "lbfgs"</kbd>.
	  </p><div class="theUglyTrick"><br><br></div>
	</div>
      </div>
    </div>

  </p>

  <h2>Expérimentation</h2>

  <p>

    Vous résolvez les deux problèmes indiqués plus haut, le pendule inversé et la voiture dans la montagne. Pour chacun, vous essayez d'obtenir les meilleures performances de votre algorithme. Il y a beaucoup de paramètres et de détails qui peuvent être modifiés et qui influencent les performances de l'algorithme.

    <br>

    Lorsque votre algorithme fonctionne, réalisez une vingtaine d'épisodes de test, c'est-à-dire que vous utilisez l'estimation de la qualité \( \hat{Q} \) et que vous exécutez 20 fois la politique gloutonne. À chaque exécution, vous mesurez le retour, c'est-à-dire la somme des retours immédiats pondérés, \( R = \sum_t \gamma^t r_t \). Quelle est la médiane&nbsp;? Quelle est la variabilité de \( R \)&nbsp;?
    
  </p>

  <h3>Conseils</h3>

<p>

  L'utilisation de réseau de neurones dans un algorithme d'apprentissage par renforcement n'est pas si simple. On l'a vu précédemment, le Q-Learning tabulaire est sensible aux paramètres de l'algorithme et à la manière de le mettre en &oelig;uvre. Quand on remplace la table par un réseau de neurones, la sensibilité augmente. Par exemple, la première question que l'on se pose est&nbsp;: quelle architecture choisir pour le MLP&nbsp;? Je conseille de suivre l'exemple de M. Riedmiller explicité dans son papier cité plus haut&nbsp;: deux couches cachées de 5 neurones chacune, fonction d'activation tangente hyperbolique et un neurone dans la couche de sortie avec une fonction d'activation linéaire. D'une mannière générale, si on ne vous donne pas de conseil, il faut tester plusieurs architectures en réalisant une simple tâche de régression. Dans le cas présent, vous collectez un premier \( {\cal D}_0 \) et vous entraînez un régresseur constitué d'une seule couche cachée avec quelques neurones seulement (5 par exemple, attention la valeur par défaut dans <kbd>scikit_learn</kbd> (100) est TOTALEMENT RIDICULE). Si l'erreur est importante, c'est que votre MLP n'est pas assez gros. Vous ajoutez une deuxième couche, <i>etc</i>. Si avec 5 couches cachées l'erreur est toujours importante, il y a un autre problème. Reprendre la même démarche en utilisant 1? puis 2, puis ... avec plus de neurones par couche.

  <br/>

  Si ça ne marche toujours pas, il faut regarder les paramètres de l'algorithme d'apprentissage. En utilisant lbfgs, il n'y a pas de paramètre. Simplement, cet algorithme peut être piégé dans un mauvais minimum local mais tant que le jeu de données est petit et le réseau est lui-même petit, en général, lbfgs fonctionne bien.

  <br/>

  Pour ce TP, on fait des choses simples, donc un petit réseau doit fonctionner sans difficulté. Et en général, en apprentissage par renforcement, on utilise toujours de petits réseaux de neurones. On entend souvent parler de <i>deep reinforcement learning</i>, c'est une dénommination trompeuse&nbsp;: l'apprentissage par renforcement n'est jamais profond (au sens du <i>deep learning</i>, apprentissage profond). On utilise des réseaux un peu moins superficiels (10 à 15 couches) quand l'état est une donnée complexe, comme une image. Mais dans ce cas, la plupart des couches servent à calculer une représentation de l'image qui est ensuite traitée par un MLP peu profond lequel est le réseau qui est utilisé dans l'algorithme d'apprentissage par renforcement.
  
</p>

  <h1></h1>
  
  <p>

    <span class="alert">À l'issue de ce TP, votre programme doit pouvoir être appliqué à n'importe quel problème de décision de Markov à espace d'états continu.</span>
    
  </p>
  
</div>

</body>
</html>
