<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" >
    <title>Apprentissage par différence temporelle</title>
    <link href="/home/ppreux/philippe-preux.github.io/css/ma.css"
	  rel="stylesheet" type="text/css" media="all" >
    <link href="https://philippe-preux.github.io/css/ma.css"
	  rel="stylesheet" type="text/css" media="all" >
    <link rel="shortcut icon" type="image/x-icon"
	  href="https://philippe-preux.github.io/img/site.ico" >
    <!--link rel="shortcut icon" type="image/x-icon"
	  href="/home/ppreux/philippe-preux.github.io/img/site.ico" -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
      div.c {
	  background-color: #bafcba;
      }
      div.python {
	  background-color: #c0c0c0;
      }
      table.tr.td.c {
	  background-color: #bafcba;
	  width: 50%;
      }
      table.tr.td.python {
	  background-color: #c0c0c0;
	  width: 50%;
      }
      span.alert {
	  color: #ff0000;
	  font-size: 300%;
      }

      /* Ce qui suit fait ce que je voulais faire en mettant des attributs aux balises td, ce qui n'est pas compatible avec HTML5. Je voulais une table avec deux colonnes de même largeur, chacune avec une couleur de fond particulière.
       J'ai trouvé cette solution là : https://www.w3schools.com/howto/howto_css_two_columns.asp */
      * {
	  box-sizing: border-box;
      }
      /* Create two equal columns that floats next to each other */
      .columnC {
	  background-color: #bafcba;
	  float: left;
	  width: 50%;
	  padding: 10px;
      }
      .columnP {
	  background-color: #c0c0c0;
	  float: left;
	  width: 50%;
	  padding: 10px;
      }
      /* Clear floats after the columns */
      .row:after {
	  content: "";
	  display: table;
	  clear: both;
      }
      /* ça marche bien pour la largeur et la couleur de fond, mais pas pour
	 la hauteur. Après avoir qwanté sans trouver de solution, j'ai défini
	 la classe div ci-dessous : c'est affreux, mais ça marche comme
	 disent les étudiants ! (-(
	 Mais il faut faire l'ajustement à la main. C'est bien pour cela que
	 « non, ça ne marche pas !», mais ça rend ce que je voulais donc en
	 attendant de trouver mieux.
     */
      div.theUglyTrick {
	  visibility:hidden;
      }
    </style>
  </head>
  
<body>

<div class="tpR">

  <h1>Apprentissage par différence temporelle</h1>

  <h2>Introduction</h2>

  <p>

    Principal algorithme d'apprentissage par renforcement utilisant la différence temporelle, nous étudions le Q-Learning.

  </p>

  <h2>Schéma algorithmique du Q-Learning</h2>

  <p>

    Dans le cas d'une tâche se terminant, le Q-Learning consiste à itérer des épisodes. Un épisode est une séquence d'interactions qui se poursuit jusqu'à atteindre un état terminal. Une interaction consiste à déterminer une action à réaliser dans l'état courant, l'effectuer, observer l'état atteint et le retour perçu, et enfin, mettre à jour l'estimation de la fonction Q. Un peu plus précisément, le Q-Learning est décrit ci-dessous&nbsp;:

  </p>

  <ol>
    <li>Initialiser l'algorithme.</li>
    <li><b>Répéter&nbsp;:</b>
      <ol>
	<li>/* on réalise un épisode. */</li>
	<li>Initialiser l'état ; \( t \gets 0 \)</li>
	<li><b>Tant-que \( e_t \) n'est pas terminal&nbsp;:</b>
	  <ol>
	    <li>/* on fait une interaction&nbsp;: */</li>
	    <li>en fonction de l'état courant e<sub>t</sub>, choisir l'action a<sub>t</sub>,</li>
	    <li>effectuer a<sub>t</sub> et observer l'état suivant e<sub>t+1</sub> et le retour immédiat r<sub>t</sub>,</li>
	    <li>mettre à jour Q&nbsp;: \( Q (e_t, a_t) \gets Q (e_t, a_t) + \alpha (e_t, a_t) [r_t + \gamma \max_{a' \in {\cal A}} Q (e_{t+1}, a') - Q (e_t, a_t) ] \),
	    <li>\( t \gets t + 1 \).</li>
	  </ol>
      </ol>
    </li>
  </ol>

  <h2>Implantation</h2>

  <p>

    <b>À faire&nbsp;:</b> implanter le Q-Learning. Tout comme dans le TP de programmation dynamique en PDI, faire en sorte que l'implantation soit générique. Assurez-vous qu'elle n'est pas buggée.

    <br>

    On prendra \( \alpha (e, a) = \frac{1}{n (e, a) + 1} \) où \( n (e, a) \) est le nombre de visites à la paire \( (e, a) \).

    <br>

    On utilisera un choix d'action &epsilon;-glouton avec &epsilon; décroissant. La décroissance de &epsilon; est délicate à régler. On multipliera &epsilon; par une valeur proche et inférieure à 1 à l'issue de chaque épisode, par exemple 0,99. Par la suite dans les expériences, on vérifiera toujours que ce facteur convient et ne provoque pas une décroissance d'&epsilon; trop rapide, ou trop lente.

  </p>

  <h2>Expérimentation</h2>

  <p>

    <span class="alertje">Vos expériences doivent être reproductibles.</span>
    
  </p>

  <h3>Problème du chauffeur de taxi</h3>

  <p>

    Résoudre le probème du chauffeur de taxi avec le Q-Learning. On prend &gamma; = 0,9. Comme il n'y a pas d'état terminal, on arrête un épisode lorsque &gamma;<sup>t</sup> est petit (10<sup>-6</sup> par exemple).

    <br>

    Vérifier que la politique gloutonne tend vers la politique optimale déterminée dans le module TP de programmation dynamique. Combien d'épisodes sont nécessaires pour cela&nbsp;?

    <br/>

    Vous étudierez par exemple combien d'interactions sont nécessaires pour apprendre cette politique. Exécutez plusieurs fois l'algorithme, calculez la moyenne, la médiane et l'écart-type.

  </p>
  
  <h3>Problème de labyrinthes</h3>

  <p>

    Résoudre le labyrinthe dont \( {\cal P} \) et \( {\cal R} \) sont fournies dans <a href="laby.1bis.P-et-R.txt">ce fichier</a>. 4 actions sont possibles, une par direction cardinale&nbsp;: aller une case à gauche, aller une case vers le haut, aller une case vers la droite, aller une case vers le bas. Lorsqu'une action est empéchée par un mur, l'état reste inchangé. Ce fichier suit le format vu dans le TP de programmation dynamique. Chaque épisode part de l'état orange et se termine lorsque l'état vert est atteint. Un retour immédiat +1 est perçu lorsque la case verte est atteinte&nbsp;; la fonction de retour est nulle sur toutes les autres transitions&nbsp;; une fois sur la case verte, toutes les actions sont sans effet. Les états sont numérotés à partir de 0 pour la case en haut à gauche puis de gauche vers la droite et de haut en bas. Déterminer la politique gloutonne à l'issue de l'apprentissage. Lors d'une exécution, on mesure la somme des retours immédiats pondérés \( R = \sum_{t\ge{}0} \gamma^t r_t \). Prendre &gamma; = 0,95.

    <br>

    <img src="https://philippe-preux.github.io/ensg/miashs/rnf/tps/rl/laby.1bis.svg" width="300" alt="(figure) Un petit labyrinthe.">
    
    <br>

    On nomme courbe d'apprentissage le graphique indiquant \( R \) en fonction du numéro de l'épisode.

    <br>
    
    Réalisez plusieurs exécutions. On réalise plusieurs courbes d'apprentissage &nbsp;:

    <br>

    Pour vous donner quelques repères&nbsp;: en réalisant 12 épisodes comprenant 200 interactions chacun au maximum, avec &gamma; = 0,9 et facteur de décroissance de &epsilon; = 0,98, mon implantation du Q-Learning réalise une trajectoire optimale.

  </p>

  <ul>
    <li>version 1&nbsp;: tracer l'évolution de \( R \) au fil des épisodes pour chacune des exécutions.</li>
    <li>version 2&nbsp;: tracer la valeur moyenne de \( R \) au fil des épisodes et +/- 1 écart-type.</li>
    <li>version 3&nbsp;: tracer la valeur médiane de \( R \) au fil des épisodes et les quartiles 25 et 75%.</li>
  </ul>

  <p>

    Faites la même chose avec le labyrinthe du contrôle de PDI (version non venteuse)&nbsp;: <a href="../../../PDI/labyrinthe-P-et-R.txt">labyrinthe du CC de PDI</a>.

    Pour vous donner quelques repères&nbsp;: en réalisant 25 épisodes comprenant 750 interactions chacun au maximum, avec &gamma; = 0,9 et facteur de décroissance de &epsilon; = 0,98, mon implantation du Q-Learning réalise une trajectoire optimale.

  </p>

  <h3>21 avec un dé</h3>

  <p>

    Résolvez le 21 avec un dé avec le Q-Learning.
    
  </p>
  
  <h3>Choix de l'action</h3>

  <p>

    Implanter la méthode de Botzmann-Gibbs. Comparez les performances obtenues avec celle-ci à celles obtenues précédemment par &epsilon;-décroissant glouton.

  </p>

  <h2>Traces d'éligibilité</h2>

  <p>

    Implanter l'algorithme Q (&lambda;) et le tester sur ces 3 problèmes. Q(&lambda;) est spécifié par l'algorithme 6 du polycopié.

    <br>

    Comparer les performances du Q-learning avec le Q(&lambda;).
    
  </p>
  
  <h1></h1>
  
  <p>

    <span class="alert">À l'issue de ce TP, votre Q-Learning et votre Q(&lambda;) doivent pouvoir être appliqués à n'importe quel problème de décision de Markov.</span>
    
  </p>
  
</div>

</body>
</html>
