<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr" lang="fr">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Recherche des poids d'un perceptron non linéaire</title>
  <link href="https://philippe-preux.github.io/css/ma.css" 
	rel="stylesheet" type="text/css" media="all" />
  <link rel="shortcut icon" type="image/x-icon" 
	href="https://philippe-preux.github.io/img/site.ico" />
</head>

<body>
<div class="tpR">

  <h1>Recherche des poids d'un perceptron non linéaire</h1>

  <p>

    Ce TP reprend le TP précédent mais il est cette fois appliqué à un perceptron dont la fonction d'activation est une fonction sigmoïde.

    <br/>

    Comme en cours, on considère un perceptron réel à sortie réelle. La fonction d'activation est soit la fonction logistique, soit la fonction tangente hyperbolique.

  </p>

  <h2>Fonction d'activation logistique</h2>
  
  <h3>DGS pour calculer les poids d'un perceptron à fonction d'activation logistique</h3>
  
  <p>

    On adapte la DGS vue dans le TP précédent à la fonction d'activation logistique. Ci-dessous, j'indique ce qui change en <font color="red">rouge</font>.

  </p>

  <ul>
    <li>On a un jeu d'exemples constitué de N données d'entrées et des N valeurs de sortie attendues. Chaque donnée est décrite par P attributs.
      <br/>
      On note x<sub>i</sub> la i<sup>è</sup> donnée et x<sub>i,j</sub> l'attribut j de la i<sup>è</sup> donnée. On note y<sub>i</sub> la sortie attendue pour la i<sup>è</sup> donnée. Le i<sup>è</sup> exemple est donc le couple (x<sub>i</sub>, y<sub>i</sub>).</li>
    <li>On effectue la descente de gradient stochastique&nbsp;:
      <ol>
	<li><font color="red">centrer et réduire les exemples.</font></li>
	<li>Initialiser les P poids avec une valeur quelconque.</li>
	<li><font color="red">corrections = 1</font></li>
	<li><b>Tant-que</b> <font color="red">|corrections| &gt; &epsilon;</font><b>:</b>
	  <ol>
	    <li><font color="red">corrections = 0</font></li>
	    <li><b>Pour</b> chaque exemple x<sub>i</sub><b>:</b>
	      <ol>
		<li>on calcule la sortie s du perceptron pour cet exemple.</li>
		<li><font color="red">On en déduit la classe prédite&nbsp;: <b>si</b> s &ge; 1/2, <b>alors</b> classe.prédite = 1, <b>sinon</b> classe.prédite = 0.</font></li>
		<li>d = <font color="red">classe.prédite</font> - y<sub>i</sub>
		<li><b>Si</b> d est non nulle <b>alors</b>
		  <br/>
		  <ol>
		    <li>biais = biais - &eta; d <font color="red">s (1 - s)</font></li>
		    <li><font color="red">corrections = corrections + |&eta; d s (1 - s)|</font></li>
		    <li><b>Pour</b> chaque poids j <b>:</b>
		      <ul>
			<li>p<sub>j</sub> = p<sub>j</sub> - &eta; d x<sub>i,j</sub><font color="red"> s (1 - s)</font></li>
			<li><font color="red">corrections = corrections + |&eta; d x<sub>i,j</sub> s (1 - s)|</font></li>
		      </ul>
		    </li>
		  </ol>
		</li>
		</ol>
	      </li>
	  </ol></li>
      </ol></li>
    <li>C'est fini.</li>
    </ul>

    <p>

      Nous allons écrire le programme en python qui fait cela. Nous allons introduire un certain nombre de raffinements par rapport à ce que nous avions fait dans le TP précédent.

    </p>

    <h3>Quelques éléments pour la mise en &oelig;uvre</h3>

    <p>

      On précise quelques points nouveaux concernant l'écriture du programme en python et l'implantation de cette nouvelle version de la DGS.

    </p>
    
    <h4>Matrice de données</h4>

    <p>

    Dans les TPs précédents, nous avons considéré que les données sont contenues dans une liste. Dans la réalité, on les stocke dans une matrice dans laquelle chaque ligne correspond à une donnée et chaque colonne à un attribut.

    <br/>

    Dans le TP précédent, quand nous avons utilisé le jeu de données iris, celles-ci étaient stockées dans une matrice. Je ne vous l'ai pas dit, vous ne vous en êtes pas rendu compte, ce qui montre qu'il n'y a rien de compliqué. Pour accéder à l'attribut numéro j de la donnée numéro i, on écrit <kbd>nom_de_la_matrice [i, j]</kbd>.

    <br/>

    Pour pouvoir utiliser des matrices, vous devez au préalable écrire dans votre programme <kbd>import numpy as np</kbd>.

    </p>

    <h4>Fonction d'activation</h4>

    <p>

      Il est utile de définir une fonction <kbd>logistique (v)</kbd> qui renvoie la valeur de la fonction logistique appliquée au potentiel <kbd>v</kbd>.

      <br/>
      
      La fonction logistique est définie par&nbsp;: &phi; (x) = 1 / (1 + e<sup>-x</sup>). Son graphe est&nbsp;:

      <br/>

      <img src="logistique.png" width="300pt" alt="Graphe de la fonction logistique" />

      <br/>

      Pour utiliser la fonction exponentielle <kbd>exp ()</kbd>, vous devez préalablement écrire l'instruction&nbsp;: <kbd>from math import exp</kbd>.
      
    </p>

    <h4>Classe prédite</h4>

    <p>

      Avec une fonction d'activation sigmoïde, la sortie du perceptron prend une valeur dans l'intervalle ]0, 1[. On veut interpréter cette valeur comme&nbsp;: si elle est inférieure à 0,5, cela signifie que le perceptron prédit la classe 0 pour la donnée placée sur son entrée&nbsp;; si cette valeur est &ge; 0,5, c'est que le perceptron prédit la classe 1 pour cette donnée.

      <br/>

      Il faut donc transformer la valeur en sortie du perceptron en un 0 ou un 1 pour pouvoir comparer la prédiction du perceptron avec l'étiquette de l'exemple.
      
    </p>

    <h4>Centrer et réduire les attributs des données</h4>

    <p>

      On l'a vu en cours, centrer un attribut signifie retirer la moyenne à chacune de ses valeurs et le réduire consiste à diviser l'attribut par l'écart-type de l'attribut centré.

      <br/>

      Cette opération est essentielle si on veut que la DGS fonctionne bien.

      <br/>

      On peut écrire soi-même une fonction pour le faire. C'est un bon exercice.

      <br/>

      Cette opération est tellement importante qu'elle est déjà codée. On fait comme cela sur l'exemple des iris&nbsp;:

    <p>
<pre># on charge le jeu de données iris (cf. TP précédent)
from sklearn import datasets
iris = datasets.load_iris()
entrées_iris = iris.data[:,2:4]

# on centre et on réduit comme suit
from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler ()
scaler.fit (entrées_iris) # cette instruction indique que l'on veut centrer et réduire les données se trouvant dans la matrice dénommé entrées_iris.
entrées = scaler.transform (entrées_iris) # cette instruction effectue le centrage et la réduction.</pre>

    <p>

      <kbd>entrées</kbd> est une matrice qui contient les iris décrits par leurs 4 attributs centrés et réduits.

    </p>

    <h3>Réalisation</h3>
    
    <p>

      On a maintenant tous les ingrédients pour réaliser la DGS. Pour s'assurer que le programme fonctionne bien dans un cas simple, on commence par le cas où le perceptron peut correctement calculer la sortie attendue. Une fois assuré que le programme fonctionne bien dans ce cas-là, on vérifiera qu'il fonctionne aussi lorsqu'il ne peut pas calculer correctement la valeur attendue quelle que soit la donnée en entrée.

      <br/>

      Comme dans le TP précédent, vous utiliserez les lignes de python que je vous avez indiquées pour réaliser des graphiques.

      <br/>

      Je résume les différents éléments expliqués ci-dessus. Vous n'avez plus qu'à compléter avec la DGS proprement dite, laquelle ressemble à ce que vous avez écrit lors du précédent TP.
      
    </p>

    <pre>'''
commentaire habituel en début de programme.

Je vous laisse compléter...
'''

# Les fonctions permettant de créer le graphique
import matplotlib.pyplot as plt

def debut_figure (entrées, sorties):
    fig, ax = plt.subplots ()
    couleurs = []
    for i in range (len (sorties)):
        if sorties [i] == 1:
            couleurs.append ("red")
        else:
            couleurs.append ("blue")
    for i in range (len (sorties)):
        ax.scatter (entrées [i, 0], entrées [i, 1], color = couleurs [i], s = 2)
    return fig, ax

def ajoute_droite (ax, a, b, c, xlim = [-2, 2]):
    x1 = xlim
    y1 = [-(xlim[0]*b+a)/c, -(xlim[1]*b+a)/c]
    ax.set_ylim (bottom = -5, top = 5)
    ax.plot (x1, y1, linestyle = ":")

def ajoute_dernière_droite (fig, ax, a, b, c, xlim = [-2, 2]):
    x1 = xlim
    y1 = [-(xlim[0]*b+a)/c, -(xlim[1]*b+a)/c]
    ax.set_ylim (bottom = -5, top = 5)
    ax.plot (x1, y1, color = "red", linestyle = "-", linewidth= 3)
    fig.show ()

# Je charge le jeu de données iris (comme dans le TP précédent).
from sklearn import datasets
iris = datasets.load_iris()
entrées_iris = iris.data[:,2:4]

# Il est essentiel de centrer-réduire !!!!!
# On fait comme cela :
from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler ()
scaler.fit (entrées_iris)
entrées = scaler.transform (entrées_iris)
'''
    entrées est une matrice de 150 lignes (le nombre de données iris) et 4 colonnes (les 4 attributs).
    Les 4 attributs sont maintenant centrés et réduits.
'''

# Je calcule la sortie attendue pour chaque donnée.
sorties_iris = []
for i in range (len (iris.target)):
    if iris.target [i] == 0:
        sorties_iris.append (0)
    else:
        sorties_iris.append (1)
sorties = sorties_iris

# J'ai besoin d'utiliser la fonction exp(), donc j'écris :
from math import exp

# On définit la fonction logistique (je vous laisse le faire) :
def logistique (v):
    ...

# On va maintenant réaliser la DGS
'''
    On commence par initialiser les poids. Vous initialisez avec les valeurs
    que vous voulez mais si vous voulez obtenir exactement le même résultat
    que moi, il faut les initialiser comme cela :
'''
a = 1
b = -3
c = 0

'''
    Tout est en place.
    Maintenant, vous écrivez la DGS.



'''</pre>

    <h4>Distinguer une classe séparable</h3>

    <p>

      Lancez votre programme python. Dans la boucle pour qui égrène les exemples, comptez le nombre d'exemples mal prédits et affichez ce nombre.
      On peut prendre &eta; = 0,01 et &epsilon; = 0,001.

      <br/>
      <br/>

      Vous devez obtenir le graphique suivant&nbsp;:
      <br/>
      
      <img src="logistique1.png" width="300pt" alt="" />

      <br/>

      et le perceptron ne doit commettre aucune erreur de prédiction.
      
    </p>

    <h4>Cas où les classes ne sont pas linéairement séparables</h3>

    <p>

      Dans le cas où le perceptron ne peut pas réaliser une prédiction parfaite, le test du tant-que n'est pas adapté. Si on itère tant que la correction est supérieure à un seuil, le choix du seuil &epsilon; est compliqué&nbsp;: on risque soit de le prendre trop grand ce qui a pour conséquences que les poids sont ajustés grossiérement, donc que le nombre d'erreurs de prédiction est plus grand que ce qu'il devrait être&nbsp;; si &epsilon; est pris trop petit, on risque de boucler infiniment car la correction ne sera jamais assez petite pour satisfaire le test et quitter la boucle. On pourrait contrecarrer ce problème soit en limitant arbitrairement le nombre d'itérations du tant-que, soit en diminuant la valeur de &eta; au fil des itérations.

      <br/>
      <br/>

      Vous essayerez deux solutions différentes&nbsp;:
      
    </p>

    <ol>
      <li>compter le nombre d'itérations de la boucle tant-que et quitter la boucle par exemple après 10<sup>3</sup> itérations.</li>
      <li>Remplacer le test&nbsp;:
	<br/>
	<b>Tant-que</b> |corrections| &gt; &epsilon;<b>:</b>
	<br/>
	par le test&nbsp;:
	<br/>
	<b>Tant-que</b> |corrections_précédentes - corrections_courantes| &gt; &epsilon;</font><b>:</b>
      où corrections_courantes est la somme des corrections appliquées aux poids lors de l'itération précédente du tant-que et corrections_précédentes est la somme des corrections effectuées lors de l'itération encore précédente.
      <br/>
      L'idée du test est que l'on itère tant que les corrections diminuent significativement d'une itération du tant-que à la suivante. Il faut prendre un seuil très petit, comme &epsilon; = 10<sup>-7</sup>.
      </ol>

    <p>

      Commencez par tester ces modifications sur l'exemple précédent. Vous devez obtenir les mêmes résultats que précédemment.

      <br/>
      <br/>

      On va maintenant appliquer cette DGS à un cas où les données ne sont pas linéairement séparables. Pour cela, vous remplacez les lignes&nbsp;:
      
    </p>
    <pre>if iris.target [i] == 0:
        sorties_iris.append (0)
    else:
        sorties_iris.append (1)</pre>

    <p>

      par&nbsp;:
      
    </p>

    <pre>if iris.target [i] != 2:
        sorties_iris.append (0)
    else:
        sorties_iris.append (1)</pre>

    <p>
      uniquement ces lignes-là&nbsp;!

      <br/>

      Cette fois-ci, le perceptron doit distinguer les données qui sont en haut à droite du graphique des autres. Elles se mélangent avec celles qui se trouvent au milieu et elles ne peuvent pas être séparées par une droite.

      <br/>

      Prendre une valeur encore plus petite du seuil que plus haut, par exemple &epsilon; = 10<sup>-8</sup>.
      
      <br/>

      Vous devez obtenir ce graphique&nbsp;:

      <br/>

      <img src="logistique2c.png" width="300pt" alt="" />

      <br/>

      et le perceptron commet 6 erreurs de prédiction.
      
    </p>
    
    <h2>Fonction d'activation tangente hyperbolique</h2>

    <p>

      La tangente hyperbolique a la même forme que la fonction logistique, sauf qu'elle est dilatée selon l'axe des ordonnées. Cette petite différence change pas mal de choses quant à l'efficacité du perceptron et de la DGS.

      <br/>

      <img src="tanh.png" width="300pt" alt="graphe de la tangente hyperbolique" />

      <br/>

      Vous allez refaire tout ce qui précède en changeant simplement la fonction d'activation. Il faut que vous ayez fait ce qui précède avec la fonction logistique. Les modifications à apporter sont très très simples.

      <br/>

      Pour cela&nbsp;:
      
    </p>

    <ul>
      <li>il faut bien sûr changer le calcul de l'activation du perceptron. Pour utiliser la fonction <kbd>tanh()</kbd>, on doit auparavant effectuer l'instruction <kbd>from math import tanh</kbd> (à la place de <kbd>from math import exp</kbd>).
	<br/>
	Quand on calcule la sortie s du perceptron, on doit donc utiliser la tangente hyperbolique à la place de la fonction logistique.
      </li>
      <li>Il faut modifier la correction des poids. Le terme s (1 - s) correspond à la fonction logistique. Pour la tangente hyperbolique, il faut le remplacer par (1 - s * s).</li>
      <li>Puisque la tangente hyperbolique produit une valeur entre -1 et 1, il faut modifier la calcul de la classe prédite&nbsp;: au lieu de
	<br/>
	<b>si</b> s &ge; 1/2, <b>alors</b> classe.prédite = 1, <b>sinon</b> classe.prédite = 0.
	<br/>
	il faut comparer a à 0&nbsp;:
	<br/>
      	<b>si</b> s &ge; 0, <b>alors</b> classe.prédite = 1, <b>sinon</b> classe.prédite = 0.</li>
      <li>Avec ces trois petites modifications, votre programme devrait déjà fonctionner.</li>
      <li>Avec la tangente hyperbolique, la valeur de &epsilon; peut être bien plus grande. &epsilon;=10<sup>-5</sup> convient, ce qui permet de réaliser moins d'itérations, donc que le calcul des poids soit plus rapide.</li> <!-- 66 itérations tanh1c.py, 80 pour logistique1c.py -->
    </ul>
  
    <h2>Pour finir</h2>

    <p>

      Le source de votre programme doit respecter les points suivants&nbsp;:
      
    </p>
    
    <ul>
      <li>il doit commencer par un commentaire indiquant son titre, son objet, ses auteurs, la date de réalisation.</li>
      <li>Que vous utilisiez des fonctions ou pas (ce n'est pas obligatoire), vous commentez votre programme avec parcimonie, là où c'est utile.</li>
      <li>Chaque fonction doit commencer par un commentaire indiquant au minimum ce que fait la fonction, le sens des paramètres, les pré-conditions et ce que la fonction retourne.</li>
      <li>Le type des paramètres des fonctions doit être indiqué, ainsi que le type de la valeur retournée.</li>
    </ul>

    <p>

      Pour finir, vous m'envoyez votre/vos script(s) par email, en mettant votre binôme en cc.

    </p>

    </div>

    </body>
</html>
