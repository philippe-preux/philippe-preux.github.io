<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
	  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="fr" lang="fr">
  <head>
    <title>Model-based RL</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <!--link rel="stylesheet" type="text/css" href="./ma.css" /-->
    <style>
      body {font-family: Comic Sans MS;}
      body {background-color: #33ffff;}
      h3 {color: Tomato;}
    </style>
  </head>
  <body>

    <p>

      Internship proposal.

    </p>

    <ul>
      <li>Title: Model-based RL</li>
      <li>Supervisor: <a href="https://philippe-preux.github.io/">Philippe Preux</a></li>
      <li>Duration: 5 to 6 months</li>
      <li>When: Spring-Summer 2020</li>
      <li>Where: <a href="http://sequel.lille.inria.fr/">SequeL</a>, Inria Lille, Villeneuve d'Ascq, France</li>
      <li>Expected background: master MVA, or master in CS specialized in machine learning.</li>
      <li>Keywords: reinforcement learning, deep RL, algorithms, experimental</li>
      <li>Context: <br/>
	Reinforcement learning is a sub-field of machine learning in which we aim at designing agents that learn to act. Acting usually involves performing a sequence of actions in order to achieve a goal. Examples are countless; games are good examples, like pacman or chess in which the player has to perform a series of action either to reach a maximal score, or to defeat his opponent. Applications of RL go way beyond games.
	<br/>
	RL algorithms are inherently slow to learn. One way to make them more efficient is to make them learn a model of their environment; this model is meant to be refined along learning and the RL agent may use this model in order to learn more efficiently. This idea is not new, dating back at least to Sutton's Dyna architecture.
      </li>
      <li>What: <br/>
	The goal of this internship is:
	<ul>
	  <li>Study the litterature of model-based RL.</li>
	  <li>Explore new ideas. This exploration can be theoretical or algorithmic.</li>
	  <li>Perform an experimental assessment of the ideas.</li>
	</ul>
	The spirit is ``pratical'': in practice, if I want to learn a model, how should I do? I favor a pragmatic approach, tackling the problem with simple well-thought experiments that help understand how the agent is learning, using theory as a guide. Does the approach scale? How sensitive is it wrt stochasticity of the environment? time varying environments? ...
    <li>Bibliography:
      <ul>
	<li><a href="http://incompleteideas.net/book/the-book.html">Sutton and Barto, Reinforcement Learning, an Introduction</a></li>
	<li>Sutton, <a href="http://incompleteideas.net/publications.html#Dyna">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</a>, ICML 1990</li>
	<li>Weber <i>et al</i>., <a href="https://arxiv.org/abs/1707.06203">Imagination-Augmented Agents for Deep Reinforcement Learning</a>, 2018</li>
      </ul>
    </li>
    <li>Working environment: SequeL is a well-known research group in reinforcement learning and bandits. It is composed of 4 permanent researchers, 20+ PhD students, a couple of post-docs and engineers. SequeL provides a very rich and stimulating for doing cutting-edge research in RL.</li>
    </ul>

    <p>
      <a href="https://philippe-preux.github.io">Back to homepage.</a>
    </p>

</body>
</html>
